{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d66db50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        self.state = np.zeros((max_size, state_dim))\n",
    "        self.action = np.zeros((max_size, action_dim))\n",
    "        self.next_state = np.zeros((max_size, state_dim))\n",
    "        self.reward = np.zeros((max_size, 1))\n",
    "        self.not_done = np.zeros((max_size, 1))\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    def push(self, state, action, next_state, reward, done):\n",
    "        self.state[self.ptr] = state\n",
    "        self.action[self.ptr] = action\n",
    "        self.next_state[self.ptr] = next_state\n",
    "        self.reward[self.ptr] = reward\n",
    "        self.not_done[self.ptr] = 1. - done\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, self.size, size=batch_size)\n",
    "        return (\n",
    "            torch.FloatTensor(self.state[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.action[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.next_state[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.reward[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.not_done[ind]).to(self.device)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acf86ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ReplayBuffer at 0x1ca7b030fa0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReplayBuffer(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7a1f920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aec234a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Linear(input_dim, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, output_dim)\n",
    "        self.max_action = max_action\n",
    "        \n",
    "    def forward(self, state):\n",
    "        a = F.relu(self.l1(state))\n",
    "        a = F.relu(self.l2(a))\n",
    "        return self.max_action * torch.tanh(self.l3(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "429bf84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        # Q1 architecture\n",
    "        self.l1 = nn.Linear(input_dim + output_dim, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, 1)\n",
    "\n",
    "        # Q2 architecture\n",
    "        self.l4 = nn.Linear(input_dim + output_dim, 256)\n",
    "        self.l5 = nn.Linear(256, 256)\n",
    "        self.l6 = nn.Linear(256, 1)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        sa = torch.cat([state, action], 1)\n",
    "\n",
    "        q1 = F.relu(self.l1(sa))\n",
    "        q1 = F.relu(self.l2(q1))\n",
    "        q1 = self.l3(q1)\n",
    "\n",
    "        q2 = F.relu(self.l4(sa))\n",
    "        q2 = F.relu(self.l5(q2))\n",
    "        q2 = self.l6(q2)\n",
    "        \n",
    "        return q1, q2\n",
    "    \n",
    "    def Q1(self, state, action):\n",
    "        sa = torch.cat([state, action], 1)\n",
    "        \n",
    "        q1 = F.relu(self.l1(sa))\n",
    "        q1 = F.relu(self.l2(q1))\n",
    "        q1 = self.l3(q1)\n",
    "        \n",
    "        return q1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf51f104",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3(object):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, max_action, cfg):\n",
    "        self.max_action = max_action\n",
    "        self.gamma = cfg.gamma\n",
    "        self.lr = cfg.lr\n",
    "        self.policy_noise = cfg.policy_noise\n",
    "        self.noise_clip = cfg.noise_clip\n",
    "        self.policy_freq = cfg.policy_freq\n",
    "        self.batch_size =  cfg.batch_size \n",
    "        self.device = cfg.device\n",
    "        self.total_it = 0\n",
    "        \n",
    "        self.actor = Actor(input_dim, output_dim, max_action).to(self.device)\n",
    "        self.actor_target = copy.deepcopy(self.actor)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "\n",
    "        self.critic = Critic(input_dim, output_dim).to(self.device)\n",
    "        self.critic_target = copy.deepcopy(self.critic)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "        self.memory = ReplayBuffer(input_dim, output_dim)\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(self.device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "    \n",
    "    def update(self):\n",
    "        self.total_it += 1\n",
    "\n",
    "        # Sample replay buffer \n",
    "        state, action, next_state, reward, not_done = self.memory.sample(self.batch_size)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Select action according to policy and add clipped noise\n",
    "            noise = (\n",
    "                torch.randn_like(action) * self.policy_noise\n",
    "            ).clamp(-self.noise_clip, self.noise_clip)\n",
    "\n",
    "            next_action = (\n",
    "                self.actor_target(next_state) + noise\n",
    "            ).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "            # Compute the target Q value\n",
    "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            target_Q = reward + not_done * self.gamma * target_Q\n",
    "\n",
    "        # Get current Q estimates\n",
    "        current_Q1, current_Q2 = self.critic(state, action)\n",
    "\n",
    "        # Compute critic loss\n",
    "        critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "\n",
    "        # Optimize the critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Delayed policy updates\n",
    "        if self.total_it % self.policy_freq == 0:\n",
    "\n",
    "            # Compute actor losse\n",
    "            actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "            \n",
    "            # Optimize the actor \n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            # Update the frozen target models\n",
    "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                target_param.data.copy_(self.lr * param.data + (1 - self.lr) * target_param.data)\n",
    "\n",
    "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                target_param.data.copy_(self.lr * param.data + (1 - self.lr) * target_param.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0352553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "curr_path = os.path.dirname(os.path.realpath('__file__'))\n",
    "parent_path=os.path.dirname(curr_path) \n",
    "sys.path.append(parent_path) # add current terminal path to sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7814261c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\hoho_gym\\lib\\site-packages\\seaborn\\rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(mpl.__version__) >= \"3.0\":\n",
      "D:\\ProgramData\\Anaconda3\\envs\\hoho_gym\\lib\\site-packages\\setuptools\\_distutils\\version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from common.utils import save_results,make_dir,plot_rewards\n",
    "\n",
    "curr_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") # obtain current time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "162227fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3Config:\n",
    "    def __init__(self) -> None:\n",
    "        self.algo_name = 'TD3'\n",
    "        self.env_name = 'Pendulum-v1'#'HalfCheetah-v2'\n",
    "        self.seed = 0\n",
    "        self.result_path = curr_path+\"/results/\" +self.env_name+'/'+curr_time+'/results/'  # path to save results\n",
    "        self.model_path = curr_path+\"/results/\" +self.env_name+'/'+curr_time+'/models/'  # path to save models\n",
    "        self.start_timestep = 25e3 # Time steps initial random policy is used\n",
    "        self.eval_freq = 5e3 # How often (time steps) we evaluate\n",
    "        # self.train_eps = 800\n",
    "        self.max_timestep = 4000000 # Max time steps to run environment\n",
    "        self.expl_noise = 0.1 # Std of Gaussian exploration noise\n",
    "        self.batch_size = 256 # Batch size for both actor and critic\n",
    "        self.gamma = 0.99 # gamma factor\n",
    "        self.lr = 0.0005 # Target network update rate \n",
    "        self.policy_noise = 0.2 # Noise added to target policy during critic update\n",
    "        self.noise_clip = 0.5  # Range to clip target policy noise\n",
    "        self.policy_freq = 2 # Frequency of delayed policy updates\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.save = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c09e7185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(env,agent, seed, eval_episodes=10):\n",
    "    eval_env = gym.make(env)\n",
    "    eval_env.seed(seed + 100)\n",
    "    avg_reward = 0.\n",
    "    for _ in range(eval_episodes):\n",
    "        state, done = eval_env.reset(), False\n",
    "        while not done:\n",
    "            # eval_env.render()\n",
    "            action = agent.choose_action(np.array(state))\n",
    "            state, reward, done, _ = eval_env.step(action)\n",
    "            avg_reward += reward\n",
    "    avg_reward /= eval_episodes\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
    "    print(\"---------------------------------------\")\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ec22d446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg,env,agent):\n",
    "    # Evaluate untrained policy\n",
    "    evaluations = [eval(cfg.env_name,agent, cfg.seed)]\n",
    "    state, done = env.reset(), False\n",
    "    ep_reward = 0\n",
    "    ep_timesteps = 0\n",
    "    episode_num = 0\n",
    "    rewards = []\n",
    "    ma_rewards = [] # moveing average reward\n",
    "    for t in range(int(cfg.max_timestep)):\n",
    "        ep_timesteps += 1\n",
    "        # Select action randomly or according to policy\n",
    "        if t < cfg.start_timestep:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = (\n",
    "                agent.choose_action(np.array(state))\n",
    "                + np.random.normal(0, max_action * cfg.expl_noise, size=action_dim)\n",
    "            ).clip(-max_action, max_action)\n",
    "        # Perform action\n",
    "        next_state, reward, done, _ = env.step(action) \n",
    "        done_bool = float(done) if ep_timesteps < env._max_episode_steps else 0\n",
    "        # Store data in replay buffer\n",
    "        agent.memory.push(state, action, next_state, reward, done_bool)\n",
    "        state = next_state\n",
    "        ep_reward += reward\n",
    "        # Train agent after collecting sufficient data\n",
    "        if t >= cfg.start_timestep:\n",
    "            agent.update()\n",
    "        if done: \n",
    "            # +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\n",
    "            print(f\"Episode:{episode_num+1}, Episode T:{ep_timesteps}, Reward:{ep_reward:.3f}\")\n",
    "            # Reset environment\n",
    "            state, done = env.reset(), False\n",
    "            rewards.append(ep_reward)\n",
    "            # 计算滑动窗口的reward\n",
    "            if ma_rewards:\n",
    "                ma_rewards.append(0.9*ma_rewards[-1]+0.1*ep_reward)\n",
    "            else:\n",
    "                ma_rewards.append(ep_reward) \n",
    "            ep_reward = 0\n",
    "            ep_timesteps = 0\n",
    "            episode_num += 1 \n",
    "        # Evaluate episode\n",
    "        if (t + 1) % cfg.eval_freq == 0:\n",
    "            evaluations.append(eval(cfg.env_name,agent, cfg.seed))\n",
    "    return rewards, ma_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "944099e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1570.625\n",
      "---------------------------------------\n",
      "Episode:1, Episode T:200, Reward:-1748.539\n",
      "Episode:2, Episode T:200, Reward:-1343.161\n",
      "Episode:3, Episode T:200, Reward:-1698.254\n",
      "Episode:4, Episode T:200, Reward:-1407.550\n",
      "Episode:5, Episode T:200, Reward:-870.398\n",
      "Episode:6, Episode T:200, Reward:-1362.595\n",
      "Episode:7, Episode T:200, Reward:-1213.569\n",
      "Episode:8, Episode T:200, Reward:-862.322\n",
      "Episode:9, Episode T:200, Reward:-1197.198\n",
      "Episode:10, Episode T:200, Reward:-1514.470\n",
      "Episode:11, Episode T:200, Reward:-1357.127\n",
      "Episode:12, Episode T:200, Reward:-1252.634\n",
      "Episode:13, Episode T:200, Reward:-1535.197\n",
      "Episode:14, Episode T:200, Reward:-1193.046\n",
      "Episode:15, Episode T:200, Reward:-969.986\n",
      "Episode:16, Episode T:200, Reward:-888.127\n",
      "Episode:17, Episode T:200, Reward:-1561.163\n",
      "Episode:18, Episode T:200, Reward:-942.650\n",
      "Episode:19, Episode T:200, Reward:-896.034\n",
      "Episode:20, Episode T:200, Reward:-1716.117\n",
      "Episode:21, Episode T:200, Reward:-1572.768\n",
      "Episode:22, Episode T:200, Reward:-1202.459\n",
      "Episode:23, Episode T:200, Reward:-760.905\n",
      "Episode:24, Episode T:200, Reward:-886.958\n",
      "Episode:25, Episode T:200, Reward:-1225.053\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1570.625\n",
      "---------------------------------------\n",
      "Episode:26, Episode T:200, Reward:-1171.914\n",
      "Episode:27, Episode T:200, Reward:-1568.145\n",
      "Episode:28, Episode T:200, Reward:-1274.316\n",
      "Episode:29, Episode T:200, Reward:-1158.750\n",
      "Episode:30, Episode T:200, Reward:-883.656\n",
      "Episode:31, Episode T:200, Reward:-863.161\n",
      "Episode:32, Episode T:200, Reward:-1691.930\n",
      "Episode:33, Episode T:200, Reward:-1558.927\n",
      "Episode:34, Episode T:200, Reward:-1262.065\n",
      "Episode:35, Episode T:200, Reward:-1642.011\n",
      "Episode:36, Episode T:200, Reward:-1593.829\n",
      "Episode:37, Episode T:200, Reward:-1084.978\n",
      "Episode:38, Episode T:200, Reward:-857.002\n",
      "Episode:39, Episode T:200, Reward:-1311.115\n",
      "Episode:40, Episode T:200, Reward:-856.640\n",
      "Episode:41, Episode T:200, Reward:-1701.140\n",
      "Episode:42, Episode T:200, Reward:-899.144\n",
      "Episode:43, Episode T:200, Reward:-1497.004\n",
      "Episode:44, Episode T:200, Reward:-855.556\n",
      "Episode:45, Episode T:200, Reward:-1848.312\n",
      "Episode:46, Episode T:200, Reward:-1117.684\n",
      "Episode:47, Episode T:200, Reward:-1436.060\n",
      "Episode:48, Episode T:200, Reward:-1229.058\n",
      "Episode:49, Episode T:200, Reward:-869.046\n",
      "Episode:50, Episode T:200, Reward:-978.432\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1570.625\n",
      "---------------------------------------\n",
      "Episode:51, Episode T:200, Reward:-1149.511\n",
      "Episode:52, Episode T:200, Reward:-1381.869\n",
      "Episode:53, Episode T:200, Reward:-962.921\n",
      "Episode:54, Episode T:200, Reward:-1303.734\n",
      "Episode:55, Episode T:200, Reward:-1141.374\n",
      "Episode:56, Episode T:200, Reward:-987.391\n",
      "Episode:57, Episode T:200, Reward:-860.919\n",
      "Episode:58, Episode T:200, Reward:-859.389\n",
      "Episode:59, Episode T:200, Reward:-1510.984\n",
      "Episode:60, Episode T:200, Reward:-1209.959\n",
      "Episode:61, Episode T:200, Reward:-938.455\n",
      "Episode:62, Episode T:200, Reward:-1171.803\n",
      "Episode:63, Episode T:200, Reward:-1523.014\n",
      "Episode:64, Episode T:200, Reward:-1353.100\n",
      "Episode:65, Episode T:200, Reward:-1588.859\n",
      "Episode:66, Episode T:200, Reward:-1587.046\n",
      "Episode:67, Episode T:200, Reward:-890.614\n",
      "Episode:68, Episode T:200, Reward:-1544.084\n",
      "Episode:69, Episode T:200, Reward:-853.779\n",
      "Episode:70, Episode T:200, Reward:-891.941\n",
      "Episode:71, Episode T:200, Reward:-1276.218\n",
      "Episode:72, Episode T:200, Reward:-1629.035\n",
      "Episode:73, Episode T:200, Reward:-1531.787\n",
      "Episode:74, Episode T:200, Reward:-1535.772\n",
      "Episode:75, Episode T:200, Reward:-1613.477\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1570.625\n",
      "---------------------------------------\n",
      "Episode:76, Episode T:200, Reward:-1299.436\n",
      "Episode:77, Episode T:200, Reward:-1066.728\n",
      "Episode:78, Episode T:200, Reward:-1752.175\n",
      "Episode:79, Episode T:200, Reward:-882.165\n",
      "Episode:80, Episode T:200, Reward:-1627.372\n",
      "Episode:81, Episode T:200, Reward:-975.444\n",
      "Episode:82, Episode T:200, Reward:-1760.779\n",
      "Episode:83, Episode T:200, Reward:-1278.362\n",
      "Episode:84, Episode T:200, Reward:-1187.404\n",
      "Episode:85, Episode T:200, Reward:-1472.566\n",
      "Episode:86, Episode T:200, Reward:-1383.196\n",
      "Episode:87, Episode T:200, Reward:-1557.317\n",
      "Episode:88, Episode T:200, Reward:-950.772\n",
      "Episode:89, Episode T:200, Reward:-1705.786\n",
      "Episode:90, Episode T:200, Reward:-1606.815\n",
      "Episode:91, Episode T:200, Reward:-1770.376\n",
      "Episode:92, Episode T:200, Reward:-1507.790\n",
      "Episode:93, Episode T:200, Reward:-1075.166\n",
      "Episode:94, Episode T:200, Reward:-1737.331\n",
      "Episode:95, Episode T:200, Reward:-1144.692\n",
      "Episode:96, Episode T:200, Reward:-1737.746\n",
      "Episode:97, Episode T:200, Reward:-887.757\n",
      "Episode:98, Episode T:200, Reward:-924.221\n",
      "Episode:99, Episode T:200, Reward:-1072.132\n",
      "Episode:100, Episode T:200, Reward:-1194.022\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1570.625\n",
      "---------------------------------------\n",
      "Episode:101, Episode T:200, Reward:-1044.767\n",
      "Episode:102, Episode T:200, Reward:-1075.269\n",
      "Episode:103, Episode T:200, Reward:-965.146\n",
      "Episode:104, Episode T:200, Reward:-785.327\n",
      "Episode:105, Episode T:200, Reward:-1796.463\n",
      "Episode:106, Episode T:200, Reward:-1065.611\n",
      "Episode:107, Episode T:200, Reward:-1354.702\n",
      "Episode:108, Episode T:200, Reward:-1443.721\n",
      "Episode:109, Episode T:200, Reward:-1276.281\n",
      "Episode:110, Episode T:200, Reward:-1080.920\n",
      "Episode:111, Episode T:200, Reward:-1574.118\n",
      "Episode:112, Episode T:200, Reward:-1067.203\n",
      "Episode:113, Episode T:200, Reward:-1698.001\n",
      "Episode:114, Episode T:200, Reward:-1243.466\n",
      "Episode:115, Episode T:200, Reward:-1632.442\n",
      "Episode:116, Episode T:200, Reward:-1791.222\n",
      "Episode:117, Episode T:200, Reward:-1301.682\n",
      "Episode:118, Episode T:200, Reward:-1283.621\n",
      "Episode:119, Episode T:200, Reward:-1149.770\n",
      "Episode:120, Episode T:200, Reward:-1233.749\n",
      "Episode:121, Episode T:200, Reward:-880.449\n",
      "Episode:122, Episode T:200, Reward:-1844.533\n",
      "Episode:123, Episode T:200, Reward:-1708.541\n",
      "Episode:124, Episode T:200, Reward:-879.953\n",
      "Episode:125, Episode T:200, Reward:-1384.999\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1570.625\n",
      "---------------------------------------\n",
      "Episode:126, Episode T:200, Reward:-1311.159\n",
      "Episode:127, Episode T:200, Reward:-1534.120\n",
      "Episode:128, Episode T:200, Reward:-1594.741\n",
      "Episode:129, Episode T:200, Reward:-1822.078\n",
      "Episode:130, Episode T:200, Reward:-1641.723\n",
      "Episode:131, Episode T:200, Reward:-1440.818\n",
      "Episode:132, Episode T:200, Reward:-1624.920\n",
      "Episode:133, Episode T:200, Reward:-1553.850\n",
      "Episode:134, Episode T:200, Reward:-1515.070\n",
      "Episode:135, Episode T:200, Reward:-1572.352\n",
      "Episode:136, Episode T:200, Reward:-1570.558\n",
      "Episode:137, Episode T:200, Reward:-1540.360\n",
      "Episode:138, Episode T:200, Reward:-1562.793\n",
      "Episode:139, Episode T:200, Reward:-1544.211\n",
      "Episode:140, Episode T:200, Reward:-1539.058\n",
      "Episode:141, Episode T:200, Reward:-1559.345\n",
      "Episode:142, Episode T:200, Reward:-1568.964\n",
      "Episode:143, Episode T:200, Reward:-1588.262\n",
      "Episode:144, Episode T:200, Reward:-1510.337\n",
      "Episode:145, Episode T:200, Reward:-1534.432\n",
      "Episode:146, Episode T:200, Reward:-1540.641\n",
      "Episode:147, Episode T:200, Reward:-1327.616\n",
      "Episode:148, Episode T:200, Reward:-1506.979\n",
      "Episode:149, Episode T:200, Reward:-1515.587\n",
      "Episode:150, Episode T:200, Reward:-1414.977\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1504.994\n",
      "---------------------------------------\n",
      "Episode:151, Episode T:200, Reward:-1410.900\n",
      "Episode:152, Episode T:200, Reward:-1554.825\n",
      "Episode:153, Episode T:200, Reward:-1541.417\n",
      "Episode:154, Episode T:200, Reward:-1523.941\n",
      "Episode:155, Episode T:200, Reward:-1483.125\n",
      "Episode:156, Episode T:200, Reward:-1439.260\n",
      "Episode:157, Episode T:200, Reward:-1552.065\n",
      "Episode:158, Episode T:200, Reward:-1481.964\n",
      "Episode:159, Episode T:200, Reward:-1507.689\n",
      "Episode:160, Episode T:200, Reward:-1506.154\n",
      "Episode:161, Episode T:200, Reward:-1523.526\n",
      "Episode:162, Episode T:200, Reward:-1505.547\n",
      "Episode:163, Episode T:200, Reward:-1486.297\n",
      "Episode:164, Episode T:200, Reward:-1190.087\n",
      "Episode:165, Episode T:200, Reward:-1544.647\n",
      "Episode:166, Episode T:200, Reward:-1500.960\n",
      "Episode:167, Episode T:200, Reward:-1542.975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:168, Episode T:200, Reward:-1422.101\n",
      "Episode:169, Episode T:200, Reward:-4.938\n",
      "Episode:170, Episode T:200, Reward:-1353.578\n",
      "Episode:171, Episode T:200, Reward:-1366.534\n",
      "Episode:172, Episode T:200, Reward:-1436.047\n",
      "Episode:173, Episode T:200, Reward:-1491.715\n",
      "Episode:174, Episode T:200, Reward:-1416.136\n",
      "Episode:175, Episode T:200, Reward:-1524.792\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1453.684\n",
      "---------------------------------------\n",
      "Episode:176, Episode T:200, Reward:-1471.792\n",
      "Episode:177, Episode T:200, Reward:-1460.349\n",
      "Episode:178, Episode T:200, Reward:-1302.874\n",
      "Episode:179, Episode T:200, Reward:-1521.424\n",
      "Episode:180, Episode T:200, Reward:-1526.669\n",
      "Episode:181, Episode T:200, Reward:-1376.724\n",
      "Episode:182, Episode T:200, Reward:-1321.212\n",
      "Episode:183, Episode T:200, Reward:-2.306\n",
      "Episode:184, Episode T:200, Reward:-1346.477\n",
      "Episode:185, Episode T:200, Reward:-1.928\n",
      "Episode:186, Episode T:200, Reward:-1504.059\n",
      "Episode:187, Episode T:200, Reward:-1258.425\n",
      "Episode:188, Episode T:200, Reward:-1513.707\n",
      "Episode:189, Episode T:200, Reward:-1319.821\n",
      "Episode:190, Episode T:200, Reward:-1345.459\n",
      "Episode:191, Episode T:200, Reward:-1320.595\n",
      "Episode:192, Episode T:200, Reward:-2.970\n",
      "Episode:193, Episode T:200, Reward:-1304.182\n",
      "Episode:194, Episode T:200, Reward:-1331.931\n",
      "Episode:195, Episode T:200, Reward:-1357.073\n",
      "Episode:196, Episode T:200, Reward:-1271.785\n",
      "Episode:197, Episode T:200, Reward:-1314.330\n",
      "Episode:198, Episode T:200, Reward:-1273.783\n",
      "Episode:199, Episode T:200, Reward:-1335.248\n",
      "Episode:200, Episode T:200, Reward:-1511.529\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1331.502\n",
      "---------------------------------------\n",
      "Episode:201, Episode T:200, Reward:-1513.977\n",
      "Episode:202, Episode T:200, Reward:-1307.457\n",
      "Episode:203, Episode T:200, Reward:-1457.816\n",
      "Episode:204, Episode T:200, Reward:-0.883\n",
      "Episode:205, Episode T:200, Reward:-1270.046\n",
      "Episode:206, Episode T:200, Reward:-1241.109\n",
      "Episode:207, Episode T:200, Reward:-1241.751\n",
      "Episode:208, Episode T:200, Reward:-1228.876\n",
      "Episode:209, Episode T:200, Reward:-1194.074\n",
      "Episode:210, Episode T:200, Reward:-1224.019\n",
      "Episode:211, Episode T:200, Reward:-1186.307\n",
      "Episode:212, Episode T:200, Reward:-1517.755\n",
      "Episode:213, Episode T:200, Reward:-1205.682\n",
      "Episode:214, Episode T:200, Reward:-1227.680\n",
      "Episode:215, Episode T:200, Reward:-1513.782\n",
      "Episode:216, Episode T:200, Reward:-1261.141\n",
      "Episode:217, Episode T:200, Reward:-1195.934\n",
      "Episode:218, Episode T:200, Reward:-1.971\n",
      "Episode:219, Episode T:200, Reward:-1249.706\n",
      "Episode:220, Episode T:200, Reward:-1509.567\n",
      "Episode:221, Episode T:200, Reward:-0.123\n",
      "Episode:222, Episode T:200, Reward:-1158.522\n",
      "Episode:223, Episode T:200, Reward:-1161.484\n",
      "Episode:224, Episode T:200, Reward:-1193.136\n",
      "Episode:225, Episode T:200, Reward:-1163.649\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1219.584\n",
      "---------------------------------------\n",
      "Episode:226, Episode T:200, Reward:-1149.911\n",
      "Episode:227, Episode T:200, Reward:-1514.066\n",
      "Episode:228, Episode T:200, Reward:-2.008\n",
      "Episode:229, Episode T:200, Reward:-1055.791\n",
      "Episode:230, Episode T:200, Reward:-1127.120\n",
      "Episode:231, Episode T:200, Reward:-1108.509\n",
      "Episode:232, Episode T:200, Reward:-2.288\n",
      "Episode:233, Episode T:200, Reward:-1511.361\n",
      "Episode:234, Episode T:200, Reward:-1034.017\n",
      "Episode:235, Episode T:200, Reward:-1018.506\n",
      "Episode:236, Episode T:200, Reward:-1512.953\n",
      "Episode:237, Episode T:200, Reward:-1126.519\n",
      "Episode:238, Episode T:200, Reward:-1098.642\n",
      "Episode:239, Episode T:200, Reward:-1512.398\n",
      "Episode:240, Episode T:200, Reward:-1083.862\n",
      "Episode:241, Episode T:200, Reward:-1083.233\n",
      "Episode:242, Episode T:200, Reward:-1002.539\n",
      "Episode:243, Episode T:200, Reward:-963.109\n",
      "Episode:244, Episode T:200, Reward:-1507.963\n",
      "Episode:245, Episode T:200, Reward:-1012.004\n",
      "Episode:246, Episode T:200, Reward:-1004.053\n",
      "Episode:247, Episode T:200, Reward:-1522.182\n",
      "Episode:248, Episode T:200, Reward:-1020.235\n",
      "Episode:249, Episode T:200, Reward:-1085.629\n",
      "Episode:250, Episode T:200, Reward:-1066.082\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -1046.167\n",
      "---------------------------------------\n",
      "Episode:251, Episode T:200, Reward:-945.663\n",
      "Episode:252, Episode T:200, Reward:-1038.438\n",
      "Episode:253, Episode T:200, Reward:-942.804\n",
      "Episode:254, Episode T:200, Reward:-1026.647\n",
      "Episode:255, Episode T:200, Reward:-1013.933\n",
      "Episode:256, Episode T:200, Reward:-8.507\n",
      "Episode:257, Episode T:200, Reward:-1091.405\n",
      "Episode:258, Episode T:200, Reward:-7.740\n",
      "Episode:259, Episode T:200, Reward:-1036.813\n",
      "Episode:260, Episode T:200, Reward:-6.664\n",
      "Episode:261, Episode T:200, Reward:-271.502\n",
      "Episode:262, Episode T:200, Reward:-932.690\n",
      "Episode:263, Episode T:200, Reward:-938.130\n",
      "Episode:264, Episode T:200, Reward:-960.471\n",
      "Episode:265, Episode T:200, Reward:-933.714\n",
      "Episode:266, Episode T:200, Reward:-910.163\n",
      "Episode:267, Episode T:200, Reward:-899.586\n",
      "Episode:268, Episode T:200, Reward:-665.069\n",
      "Episode:269, Episode T:200, Reward:-814.127\n",
      "Episode:270, Episode T:200, Reward:-5.928\n",
      "Episode:271, Episode T:200, Reward:-4.661\n",
      "Episode:272, Episode T:200, Reward:-849.265\n",
      "Episode:273, Episode T:200, Reward:-859.879\n",
      "Episode:274, Episode T:200, Reward:-5.677\n",
      "Episode:275, Episode T:200, Reward:-395.980\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -968.336\n",
      "---------------------------------------\n",
      "Episode:276, Episode T:200, Reward:-937.490\n",
      "Episode:277, Episode T:200, Reward:-797.634\n",
      "Episode:278, Episode T:200, Reward:-982.943\n",
      "Episode:279, Episode T:200, Reward:-4.485\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [75]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m max_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      8\u001b[0m agent \u001b[38;5;241m=\u001b[39m TD3(state_dim, action_dim, max_action, cfg)\n\u001b[1;32m----> 9\u001b[0m rewards, ma_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [74]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(cfg, env, agent)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Train agent after collecting sufficient data\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mstart_timestep:\n\u001b[1;32m---> 29\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done: \n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode_num\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Episode T:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep_timesteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Reward:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36mTD3.update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     67\u001b[0m actor_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Update the frozen target models\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param, target_param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_target\u001b[38;5;241m.\u001b[39mparameters()):\n",
      "File \u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\hoho_gym\\lib\\site-packages\\torch\\optim\\optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\hoho_gym\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\hoho_gym\\lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[38;5;66;03m# record the step after step update\u001b[39;00m\n\u001b[0;32m    139\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 141\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m           \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m           \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m           \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\hoho_gym\\lib\\site-packages\\torch\\optim\\_functional.py:110\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    105\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(bias_correction2))\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    109\u001b[0m step_size \u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m/\u001b[39m bias_correction1\n\u001b[1;32m--> 110\u001b[0m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cfg = TD3Config()\n",
    "env = gym.make(cfg.env_name)\n",
    "torch.manual_seed(cfg.seed)\n",
    "np.random.seed(cfg.seed)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "agent = TD3(state_dim, action_dim, max_action, cfg)\n",
    "rewards, ma_rewards = train(cfg, env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3e6a6373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "75d72161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-2.0, 2.0, (1,), float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4ee6c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
