{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb79af1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b0c8b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        value = self.critic(x)\n",
    "        probs = self.actor(x)\n",
    "        dist = Categorical(probs)\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1abecceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C:\n",
    "    ''' A2C算法\n",
    "    '''\n",
    "    def __init__(self,state_dim,action_dim,cfg) -> None:\n",
    "        self.gamma = cfg.gamma\n",
    "        self.device = cfg.device\n",
    "        self.model = ActorCritic(state_dim, action_dim, cfg.hidden_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters())\n",
    "\n",
    "    def compute_returns(self,next_value, rewards, masks):   # mask用来指示是否游戏结束，相当于done\n",
    "        R = next_value\n",
    "        returns = []\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            R = rewards[step] + self.gamma * R * masks[step]\n",
    "            returns.insert(0, R)\n",
    "        return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9805be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "curr_path = os.path.dirname(os.path.realpath('__file__'))  # 当前文件所在绝对路径\n",
    "parent_path = os.path.dirname(curr_path)  # 父路径\n",
    "sys.path.append(parent_path)  # 添加路径到系统路径\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "from common.multiprocessing_env import SubprocVecEnv\n",
    "from common.utils import save_results, make_dir, plot_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0acb2aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CConfig:\n",
    "    def __init__(self) -> None:\n",
    "        self.algo_name = 'A2C'# 算法名称\n",
    "        self.env_name = 'CartPole-v1' # 环境名称\n",
    "        self.n_envs = 8 # 异步的环境数目\n",
    "        self.gamma = 0.99 # 强化学习中的折扣因子\n",
    "        self.hidden_dim = 256\n",
    "        self.lr = 1e-3 # learning rate\n",
    "        self.max_frames = 30000\n",
    "        self.n_steps = 5   # what?\n",
    "        self.device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.save = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59cbf592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_envs(env_name):\n",
    "    def _thunk():\n",
    "        env = gym.make(env_name)\n",
    "        env.seed(2)\n",
    "        return env\n",
    "    return _thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "610b2e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_env(env,model,vis=False):\n",
    "    state = env.reset()\n",
    "    if vis: env.render()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(cfg.device)\n",
    "        dist, _ = model(state)\n",
    "        next_state, reward, done, _ = env.step(dist.sample().cpu().numpy()[0])\n",
    "        state = next_state\n",
    "        if vis: env.render()\n",
    "        total_reward += reward\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def compute_returns(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f5624c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg,envs):\n",
    "    print('开始训练!')\n",
    "    print(f'环境：{cfg.env_name}, 算法：{cfg.algo_name}, 设备：{cfg.device}')\n",
    "    env = gym.make(cfg.env_name) # a single env\n",
    "    env.seed(10)\n",
    "    state_dim  = envs.observation_space.shape[0]\n",
    "    action_dim = envs.action_space.n\n",
    "    model = ActorCritic(state_dim, action_dim, cfg.hidden_dim).to(cfg.device)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    frame_idx    = 0\n",
    "    test_rewards = []\n",
    "    test_ma_rewards = []\n",
    "    state = envs.reset()\n",
    "    while frame_idx < cfg.max_frames:\n",
    "        log_probs = []\n",
    "        values    = []\n",
    "        rewards   = []\n",
    "        masks     = []\n",
    "        entropy = 0\n",
    "        # rollout trajectory(预演算法)\n",
    "        for _ in range(cfg.n_steps):\n",
    "            state = torch.FloatTensor(state).to(cfg.device)\n",
    "            dist, value = model(state)  # 神经网络预测在当前状态下，总的回报和下一个动作的概率分布\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
    "            log_prob = dist.log_prob(action)\n",
    "            entropy += dist.entropy().mean()\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(cfg.device))\n",
    "            masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(cfg.device))\n",
    "            state = next_state\n",
    "            frame_idx += 1\n",
    "            if frame_idx % 100 == 0:\n",
    "                test_reward = np.mean([test_env(env,model) for _ in range(10)])\n",
    "                print(f\"frame_idx:{frame_idx}, test_reward:{test_reward}\")\n",
    "                test_rewards.append(test_reward)\n",
    "                if test_ma_rewards:\n",
    "                    test_ma_rewards.append(0.9*test_ma_rewards[-1]+0.1*test_reward)\n",
    "                else:\n",
    "                    test_ma_rewards.append(test_reward) \n",
    "                # plot(frame_idx, test_rewards)   \n",
    "        next_state = torch.FloatTensor(next_state).to(cfg.device)\n",
    "        _, next_value = model(next_state)\n",
    "        returns = compute_returns(next_value, rewards, masks)\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns   = torch.cat(returns).detach()  # return是目标\n",
    "        values    = torch.cat(values)   # value是神经网络（critic）的预测\n",
    "        advantage = returns - values\n",
    "        actor_loss  = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "        loss = actor_loss + 0.5 * critic_loss - 0.001 * entropy\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('完成训练！')\n",
    "    return test_rewards, test_ma_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6bb1be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练!\n",
      "环境：CartPole-v1, 算法：A2C, 设备：cuda\n",
      "frame_idx:100, test_reward:20.7\n",
      "frame_idx:200, test_reward:20.7\n",
      "frame_idx:300, test_reward:23.4\n",
      "frame_idx:400, test_reward:26.9\n",
      "frame_idx:500, test_reward:16.9\n",
      "frame_idx:600, test_reward:22.3\n",
      "frame_idx:700, test_reward:18.0\n",
      "frame_idx:800, test_reward:19.8\n",
      "frame_idx:900, test_reward:18.6\n",
      "frame_idx:1000, test_reward:21.1\n",
      "frame_idx:1100, test_reward:19.3\n",
      "frame_idx:1200, test_reward:18.8\n",
      "frame_idx:1300, test_reward:19.1\n",
      "frame_idx:1400, test_reward:16.2\n",
      "frame_idx:1500, test_reward:25.7\n",
      "frame_idx:1600, test_reward:24.3\n",
      "frame_idx:1700, test_reward:31.8\n",
      "frame_idx:1800, test_reward:31.4\n",
      "frame_idx:1900, test_reward:35.5\n",
      "frame_idx:2000, test_reward:36.7\n",
      "frame_idx:2100, test_reward:39.0\n",
      "frame_idx:2200, test_reward:41.3\n",
      "frame_idx:2300, test_reward:42.2\n",
      "frame_idx:2400, test_reward:47.6\n",
      "frame_idx:2500, test_reward:45.1\n",
      "frame_idx:2600, test_reward:51.3\n",
      "frame_idx:2700, test_reward:60.9\n",
      "frame_idx:2800, test_reward:48.4\n",
      "frame_idx:2900, test_reward:57.1\n",
      "frame_idx:3000, test_reward:49.4\n",
      "frame_idx:3100, test_reward:69.3\n",
      "frame_idx:3200, test_reward:86.3\n",
      "frame_idx:3300, test_reward:123.9\n",
      "frame_idx:3400, test_reward:103.0\n",
      "frame_idx:3500, test_reward:95.4\n",
      "frame_idx:3600, test_reward:86.7\n",
      "frame_idx:3700, test_reward:113.7\n",
      "frame_idx:3800, test_reward:148.2\n",
      "frame_idx:3900, test_reward:147.4\n",
      "frame_idx:4000, test_reward:133.0\n",
      "frame_idx:4100, test_reward:72.7\n",
      "frame_idx:4200, test_reward:46.1\n",
      "frame_idx:4300, test_reward:46.0\n",
      "frame_idx:4400, test_reward:66.0\n",
      "frame_idx:4500, test_reward:76.9\n",
      "frame_idx:4600, test_reward:234.7\n",
      "frame_idx:4700, test_reward:195.1\n",
      "frame_idx:4800, test_reward:207.0\n",
      "frame_idx:4900, test_reward:99.0\n",
      "frame_idx:5000, test_reward:88.0\n",
      "frame_idx:5100, test_reward:89.7\n",
      "frame_idx:5200, test_reward:110.5\n",
      "frame_idx:5300, test_reward:88.7\n",
      "frame_idx:5400, test_reward:77.2\n",
      "frame_idx:5500, test_reward:145.1\n",
      "frame_idx:5600, test_reward:210.7\n",
      "frame_idx:5700, test_reward:135.1\n",
      "frame_idx:5800, test_reward:115.8\n",
      "frame_idx:5900, test_reward:255.8\n",
      "frame_idx:6000, test_reward:277.4\n",
      "frame_idx:6100, test_reward:200.3\n",
      "frame_idx:6200, test_reward:155.4\n",
      "frame_idx:6300, test_reward:175.4\n",
      "frame_idx:6400, test_reward:234.2\n",
      "frame_idx:6500, test_reward:334.7\n",
      "frame_idx:6600, test_reward:221.6\n",
      "frame_idx:6700, test_reward:131.3\n",
      "frame_idx:6800, test_reward:85.4\n",
      "frame_idx:6900, test_reward:99.2\n",
      "frame_idx:7000, test_reward:114.6\n",
      "frame_idx:7100, test_reward:113.2\n",
      "frame_idx:7200, test_reward:76.6\n",
      "frame_idx:7300, test_reward:89.0\n",
      "frame_idx:7400, test_reward:80.5\n",
      "frame_idx:7500, test_reward:200.8\n",
      "frame_idx:7600, test_reward:302.3\n",
      "frame_idx:7700, test_reward:300.0\n",
      "frame_idx:7800, test_reward:220.1\n",
      "frame_idx:7900, test_reward:257.3\n",
      "frame_idx:8000, test_reward:294.9\n",
      "frame_idx:8100, test_reward:304.2\n",
      "frame_idx:8200, test_reward:282.9\n",
      "frame_idx:8300, test_reward:246.0\n",
      "frame_idx:8400, test_reward:223.9\n",
      "frame_idx:8500, test_reward:159.1\n",
      "frame_idx:8600, test_reward:168.0\n",
      "frame_idx:8700, test_reward:164.8\n",
      "frame_idx:8800, test_reward:165.0\n",
      "frame_idx:8900, test_reward:164.0\n",
      "frame_idx:9000, test_reward:176.2\n",
      "frame_idx:9100, test_reward:138.5\n",
      "frame_idx:9200, test_reward:145.8\n",
      "frame_idx:9300, test_reward:162.4\n",
      "frame_idx:9400, test_reward:151.9\n",
      "frame_idx:9500, test_reward:207.8\n",
      "frame_idx:9600, test_reward:167.4\n",
      "frame_idx:9700, test_reward:159.9\n",
      "frame_idx:9800, test_reward:160.5\n",
      "frame_idx:9900, test_reward:168.3\n",
      "frame_idx:10000, test_reward:184.9\n",
      "frame_idx:10100, test_reward:209.9\n",
      "frame_idx:10200, test_reward:201.3\n",
      "frame_idx:10300, test_reward:136.7\n",
      "frame_idx:10400, test_reward:226.9\n",
      "frame_idx:10500, test_reward:172.4\n",
      "frame_idx:10600, test_reward:187.2\n",
      "frame_idx:10700, test_reward:181.8\n",
      "frame_idx:10800, test_reward:178.2\n",
      "frame_idx:10900, test_reward:198.2\n",
      "frame_idx:11000, test_reward:174.4\n",
      "frame_idx:11100, test_reward:215.9\n",
      "frame_idx:11200, test_reward:259.8\n",
      "frame_idx:11300, test_reward:177.3\n",
      "frame_idx:11400, test_reward:205.5\n",
      "frame_idx:11500, test_reward:204.1\n",
      "frame_idx:11600, test_reward:181.3\n",
      "frame_idx:11700, test_reward:219.2\n",
      "frame_idx:11800, test_reward:185.0\n",
      "frame_idx:11900, test_reward:190.1\n",
      "frame_idx:12000, test_reward:202.0\n",
      "frame_idx:12100, test_reward:234.3\n",
      "frame_idx:12200, test_reward:309.9\n",
      "frame_idx:12300, test_reward:345.8\n",
      "frame_idx:12400, test_reward:345.9\n",
      "frame_idx:12500, test_reward:327.3\n",
      "frame_idx:12600, test_reward:372.8\n",
      "frame_idx:12700, test_reward:159.7\n",
      "frame_idx:12800, test_reward:150.9\n",
      "frame_idx:12900, test_reward:152.1\n",
      "frame_idx:13000, test_reward:166.9\n",
      "frame_idx:13100, test_reward:218.4\n",
      "frame_idx:13200, test_reward:217.4\n",
      "frame_idx:13300, test_reward:222.5\n",
      "frame_idx:13400, test_reward:254.9\n",
      "frame_idx:13500, test_reward:262.9\n",
      "frame_idx:13600, test_reward:337.0\n",
      "frame_idx:13700, test_reward:413.9\n",
      "frame_idx:13800, test_reward:415.4\n",
      "frame_idx:13900, test_reward:344.7\n",
      "frame_idx:14000, test_reward:309.5\n",
      "frame_idx:14100, test_reward:371.5\n",
      "frame_idx:14200, test_reward:337.2\n",
      "frame_idx:14300, test_reward:340.7\n",
      "frame_idx:14400, test_reward:363.4\n",
      "frame_idx:14500, test_reward:370.6\n",
      "frame_idx:14600, test_reward:415.5\n",
      "frame_idx:14700, test_reward:297.6\n",
      "frame_idx:14800, test_reward:334.1\n",
      "frame_idx:14900, test_reward:140.4\n",
      "frame_idx:15000, test_reward:106.4\n",
      "frame_idx:15100, test_reward:187.5\n",
      "frame_idx:15200, test_reward:150.7\n",
      "frame_idx:15300, test_reward:153.3\n",
      "frame_idx:15400, test_reward:102.4\n",
      "frame_idx:15500, test_reward:129.4\n",
      "frame_idx:15600, test_reward:159.8\n",
      "frame_idx:15700, test_reward:187.6\n",
      "frame_idx:15800, test_reward:198.3\n",
      "frame_idx:15900, test_reward:152.9\n",
      "frame_idx:16000, test_reward:158.6\n",
      "frame_idx:16100, test_reward:161.6\n",
      "frame_idx:16200, test_reward:175.6\n",
      "frame_idx:16300, test_reward:247.0\n",
      "frame_idx:16400, test_reward:239.8\n",
      "frame_idx:16500, test_reward:250.0\n",
      "frame_idx:16600, test_reward:248.5\n",
      "frame_idx:16700, test_reward:244.6\n",
      "frame_idx:16800, test_reward:201.9\n",
      "frame_idx:16900, test_reward:192.4\n",
      "frame_idx:17000, test_reward:181.3\n",
      "frame_idx:17100, test_reward:184.7\n",
      "frame_idx:17200, test_reward:191.5\n",
      "frame_idx:17300, test_reward:173.4\n",
      "frame_idx:17400, test_reward:181.5\n",
      "frame_idx:17500, test_reward:147.8\n",
      "frame_idx:17600, test_reward:153.6\n",
      "frame_idx:17700, test_reward:161.2\n",
      "frame_idx:17800, test_reward:152.4\n",
      "frame_idx:17900, test_reward:159.1\n",
      "frame_idx:18000, test_reward:157.4\n",
      "frame_idx:18100, test_reward:140.9\n",
      "frame_idx:18200, test_reward:142.9\n",
      "frame_idx:18300, test_reward:131.2\n",
      "frame_idx:18400, test_reward:149.4\n",
      "frame_idx:18500, test_reward:142.3\n",
      "frame_idx:18600, test_reward:124.8\n",
      "frame_idx:18700, test_reward:139.1\n",
      "frame_idx:18800, test_reward:169.4\n",
      "frame_idx:18900, test_reward:171.8\n",
      "frame_idx:19000, test_reward:185.8\n",
      "frame_idx:19100, test_reward:179.8\n",
      "frame_idx:19200, test_reward:162.4\n",
      "frame_idx:19300, test_reward:146.7\n",
      "frame_idx:19400, test_reward:169.8\n",
      "frame_idx:19500, test_reward:172.9\n",
      "frame_idx:19600, test_reward:157.6\n",
      "frame_idx:19700, test_reward:140.3\n",
      "frame_idx:19800, test_reward:147.4\n",
      "frame_idx:19900, test_reward:148.6\n",
      "frame_idx:20000, test_reward:164.9\n",
      "frame_idx:20100, test_reward:178.2\n",
      "frame_idx:20200, test_reward:172.5\n",
      "frame_idx:20300, test_reward:198.9\n",
      "frame_idx:20400, test_reward:188.4\n",
      "frame_idx:20500, test_reward:177.2\n",
      "frame_idx:20600, test_reward:195.5\n",
      "frame_idx:20700, test_reward:192.7\n",
      "frame_idx:20800, test_reward:182.6\n",
      "frame_idx:20900, test_reward:178.8\n",
      "frame_idx:21000, test_reward:198.1\n",
      "frame_idx:21100, test_reward:239.0\n",
      "frame_idx:21200, test_reward:205.1\n",
      "frame_idx:21300, test_reward:294.4\n",
      "frame_idx:21400, test_reward:290.7\n",
      "frame_idx:21500, test_reward:405.9\n",
      "frame_idx:21600, test_reward:318.3\n",
      "frame_idx:21700, test_reward:302.9\n",
      "frame_idx:21800, test_reward:263.7\n",
      "frame_idx:21900, test_reward:237.4\n",
      "frame_idx:22000, test_reward:231.4\n",
      "frame_idx:22100, test_reward:414.1\n",
      "frame_idx:22200, test_reward:334.7\n",
      "frame_idx:22300, test_reward:354.9\n",
      "frame_idx:22400, test_reward:369.9\n",
      "frame_idx:22500, test_reward:397.8\n",
      "frame_idx:22600, test_reward:476.1\n",
      "frame_idx:22700, test_reward:409.8\n",
      "frame_idx:22800, test_reward:485.8\n",
      "frame_idx:22900, test_reward:376.0\n",
      "frame_idx:23000, test_reward:348.7\n",
      "frame_idx:23100, test_reward:242.9\n",
      "frame_idx:23200, test_reward:239.1\n",
      "frame_idx:23300, test_reward:355.5\n",
      "frame_idx:23400, test_reward:457.1\n",
      "frame_idx:23500, test_reward:463.5\n",
      "frame_idx:23600, test_reward:489.8\n",
      "frame_idx:23700, test_reward:458.4\n",
      "frame_idx:23800, test_reward:328.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_idx:23900, test_reward:487.2\n",
      "frame_idx:24000, test_reward:414.7\n",
      "frame_idx:24100, test_reward:415.5\n",
      "frame_idx:24200, test_reward:331.6\n",
      "frame_idx:24300, test_reward:291.3\n",
      "frame_idx:24400, test_reward:293.2\n",
      "frame_idx:24500, test_reward:325.6\n",
      "frame_idx:24600, test_reward:317.2\n",
      "frame_idx:24700, test_reward:332.9\n",
      "frame_idx:24800, test_reward:380.2\n",
      "frame_idx:24900, test_reward:435.7\n",
      "frame_idx:25000, test_reward:365.2\n",
      "frame_idx:25100, test_reward:319.6\n",
      "frame_idx:25200, test_reward:284.7\n",
      "frame_idx:25300, test_reward:288.9\n",
      "frame_idx:25400, test_reward:314.1\n",
      "frame_idx:25500, test_reward:336.5\n",
      "frame_idx:25600, test_reward:416.0\n",
      "frame_idx:25700, test_reward:422.7\n",
      "frame_idx:25800, test_reward:478.4\n",
      "frame_idx:25900, test_reward:434.3\n",
      "frame_idx:26000, test_reward:423.7\n",
      "frame_idx:26100, test_reward:471.6\n",
      "frame_idx:26200, test_reward:500.0\n",
      "frame_idx:26300, test_reward:500.0\n",
      "frame_idx:26400, test_reward:491.3\n",
      "frame_idx:26500, test_reward:500.0\n",
      "frame_idx:26600, test_reward:466.0\n",
      "frame_idx:26700, test_reward:500.0\n",
      "frame_idx:26800, test_reward:482.8\n",
      "frame_idx:26900, test_reward:500.0\n",
      "frame_idx:27000, test_reward:431.9\n",
      "frame_idx:27100, test_reward:500.0\n",
      "frame_idx:27200, test_reward:469.2\n",
      "frame_idx:27300, test_reward:500.0\n",
      "frame_idx:27400, test_reward:500.0\n",
      "frame_idx:27500, test_reward:454.5\n",
      "frame_idx:27600, test_reward:500.0\n",
      "frame_idx:27700, test_reward:500.0\n",
      "frame_idx:27800, test_reward:430.7\n",
      "frame_idx:27900, test_reward:484.3\n",
      "frame_idx:28000, test_reward:500.0\n",
      "frame_idx:28100, test_reward:384.2\n",
      "frame_idx:28200, test_reward:320.3\n",
      "frame_idx:28300, test_reward:257.6\n",
      "frame_idx:28400, test_reward:211.5\n",
      "frame_idx:28500, test_reward:238.7\n",
      "frame_idx:28600, test_reward:360.7\n",
      "frame_idx:28700, test_reward:309.4\n",
      "frame_idx:28800, test_reward:475.4\n",
      "frame_idx:28900, test_reward:493.2\n",
      "frame_idx:29000, test_reward:470.5\n",
      "frame_idx:29100, test_reward:500.0\n",
      "frame_idx:29200, test_reward:486.2\n",
      "frame_idx:29300, test_reward:465.6\n",
      "frame_idx:29400, test_reward:500.0\n",
      "frame_idx:29500, test_reward:456.2\n",
      "frame_idx:29600, test_reward:410.7\n",
      "frame_idx:29700, test_reward:412.5\n",
      "frame_idx:29800, test_reward:424.3\n",
      "frame_idx:29900, test_reward:282.3\n",
      "frame_idx:30000, test_reward:165.6\n",
      "完成训练！\n"
     ]
    }
   ],
   "source": [
    "cfg = A2CConfig()\n",
    "envs = [make_envs(cfg.env_name) for i in range(cfg.n_envs)]\n",
    "envs = SubprocVecEnv(envs) \n",
    "# envs = make_envs(cfg.env_name)()\n",
    "rewards, ma_rewards = train(cfg, envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95021400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TimeLimit<OrderEnforcing<CartPoleEnv<CartPole-v1>>>>\n"
     ]
    }
   ],
   "source": [
    "test_env = make_envs(cfg.env_name)()\n",
    "print(test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6397a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<OrderEnforcing<CartPoleEnv<CartPole-v1>>>>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0976354f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
